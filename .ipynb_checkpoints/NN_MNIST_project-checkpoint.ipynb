{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c8cf9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.special\n",
    "import random\n",
    "import matplotlib.pyplot\n",
    "%matplotlib inline\n",
    "\n",
    "# neural network class definition\n",
    "# 3 layer neural network\n",
    "class neuralNetwork:\n",
    "    \n",
    "    #initialize the neural network\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        # set the number of nodes in each input, hidden and output layer\n",
    "        self.input_nodes = input_nodes\n",
    "        self.hidden_nodes = hidden_nodes\n",
    "        self.output_nodes = output_nodes\n",
    "\n",
    "        #setting the weights\n",
    "        #get the same weights each time for testing purposes\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # weights between input layer and the hidden layer\n",
    "        self.wih = np.random.normal(0.0, pow(self.input_nodes, -0.5),(self.hidden_nodes, self.input_nodes))\n",
    "        \n",
    "        # weights between the hidden layer and the output layer\n",
    "        self.who = np.random.normal(0.0, pow(self.hidden_nodes, -0.5),(self.output_nodes, self.hidden_nodes))\n",
    "        \n",
    "        #set the learning rate\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # our activation function is the sigmoid function\n",
    "        self.activation_function = lambda x: scipy.special.expit(x)\n",
    "        \n",
    "    #train the neural network\n",
    "    def train(self, inputs_list, targets_list):\n",
    "        # convert inputs list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        targets = np.array(targets_list, ndmin=2).T\n",
    "        \n",
    "        # calculate signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "\n",
    "        #calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "\n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "              \n",
    "        # calculate signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "              \n",
    "        # error is the (target - actual program output)\n",
    "        node_errors = (targets - final_outputs) \n",
    "         \n",
    "        # hidden layer error is the outputs_errors, split by the weights, recombined at the hidden nodes\n",
    "        hidden_errors = np.dot(self.who.T, node_errors)\n",
    "   \n",
    "        #update the weights for the links between the hidden and output layers\n",
    "        self.who += self.learning_rate * np.dot((node_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "        self.wih += self.learning_rate * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "        \n",
    "    #query the neural network\n",
    "    def query(self, inputs_list):\n",
    "        # convert input list to 2d array\n",
    "        inputs = np.array(inputs_list, ndmin=2).T\n",
    "        \n",
    "        # calculate the signals into hidden layer\n",
    "        hidden_inputs = np.dot(self.wih, inputs)\n",
    "        \n",
    "        #calculate the signals emerging from hidden layer\n",
    "        hidden_outputs = self.activation_function(hidden_inputs)\n",
    "        \n",
    "        # calculate signals into final output layer\n",
    "        final_inputs = np.dot(self.who, hidden_outputs)\n",
    "        \n",
    "        # calculate signals emerging from final output layer\n",
    "        final_outputs = self.activation_function(final_inputs)\n",
    "        \n",
    "        return final_outputs\n",
    "    \n",
    "    def get_wih(self):\n",
    "        return self.wih\n",
    "    \n",
    "    def get_who(self):\n",
    "        return self.who\n",
    "    \n",
    "    # Save the weights to a file\n",
    "    def save_weights(self, wih_filename, who_filename):\n",
    "        np.save(wih_filename, self.wih)\n",
    "        np.save(who_filename, self.who)\n",
    "\n",
    "    # Load weights from a file\n",
    "    def load_weights(self, wih_filename, who_filename):\n",
    "        self.wih = np.load(wih_filename)\n",
    "        self.who = np.load(who_filename)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"number of input nodes: {self.input_nodes}, number of hidden nodes: {self.hidden_nodes}, number of output nodes: {self.output_nodes}. learning rate: {self.learning_rate}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2848f517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# create an instance of neural network\n",
    "n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f156b0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first number in the list is the label (the actual label of the image)\n",
    "# the subsequent 784 numbers (28x28 pixel image) are the 0-255 color values\n",
    "# load the mnist training data CSV file into a list\n",
    "training_data_file = open(\"mnist_train.csv\", 'r')\n",
    "training_data_list = training_data_file.readlines() #list of strings\n",
    "training_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98deff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data set of 60,000 records, and the test data set of 10,000 records. \n",
    "# train the neural network\n",
    "epochs = 5\n",
    "\n",
    "for e in range(epochs):\n",
    "    # go through all records in the training data set\n",
    "    for record in training_data_list:\n",
    "        # split the record by the ',' commas\n",
    "        all_values = record.split(',')\n",
    "        # scale and shift the inputs\n",
    "        inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "        # create the target output values (all 0.01, except the desired label which is 0.99)\n",
    "        targets = np.zeros(output_nodes) + 0.01\n",
    "        # all_values[0] is the target label for this record\n",
    "        targets[int(all_values[0])] = 0.99\n",
    "        n.train(inputs, targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b5374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the mnist data CSV file into a list\n",
    "test_data_file = open(\"mnist_test.csv\", 'r')\n",
    "test_data_list = test_data_file.readlines()\n",
    "test_data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a93f636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set \n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commans\n",
    "    all_values = record.split(',')\n",
    "    # the correct answer is the first vaklue\n",
    "    correct_label = int(all_values[0])\n",
    "    #print(f\"correct label: {correct_label}\")\n",
    "    # scale and shift the inputs\n",
    "    inputs = (np.asfarray(all_values[1:])/ 255.0 * .99) + 0.01\n",
    "    # query the network\n",
    "    outputs = n.query(inputs)\n",
    "    # index with the highest value corresponds to the label\n",
    "    label = np.argmax(outputs)\n",
    "    #print(f\"network's answer {label}\")\n",
    "    if (label == correct_label):\n",
    "        scorecard.append(1)\n",
    "    else: \n",
    "        scorecard.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1fabf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print(\"performance =\", scorecard_array.sum() / len(scorecard))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab422fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n.get_wih())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a258360",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n.get_who())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd16c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n.save_weights('wih.npy', 'who.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb891e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of input, hidden and output nodes\n",
    "input_nodes = 784\n",
    "hidden_nodes = 200\n",
    "output_nodes = 10\n",
    "\n",
    "# learning rate\n",
    "learning_rate = 0.01\n",
    "\n",
    "# create an instance of neural network\n",
    "new_nn = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)\n",
    "new_nn.load_weights('wih.npy', 'who.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e552362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the neural network with the new_nn instance\n",
    "# scorecard for how well the network performs, initially empty\n",
    "scorecard = []\n",
    "\n",
    "# go through all the records in the test data set\n",
    "for record in test_data_list:\n",
    "    # split the record by the ',' commas\n",
    "    all_values = record.split(',')\n",
    "    # correct answer is first value\n",
    "    correct_label = int(all_values[0])\n",
    "    # scale and shift the inputs\n",
    "    inputs = (np.asfarray(all_values[1:]) / 255.0 * 0.99) + 0.01\n",
    "    # query the network using new_nn\n",
    "    outputs = new_nn.query(inputs)\n",
    "    # the index of the highest value corresponds to the label\n",
    "    label = np.argmax(outputs)\n",
    "    # append correct or incorrect to list\n",
    "    if (label == correct_label):\n",
    "        # network's answer matches correct answer, add 1 to scorecard\n",
    "        scorecard.append(1)\n",
    "    else:\n",
    "        # network's answer doesn't match correct answer, add 0 to scorecard\n",
    "        scorecard.append(0)\n",
    "    \n",
    "# calculate the performance score, the fraction of correct answers\n",
    "scorecard_array = np.asarray(scorecard)\n",
    "print (\"performance = \", scorecard_array.sum() / scorecard_array.size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad8d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "wih_loaded = np.load('wih.npy')\n",
    "who_loaded = np.load('who.npy')\n",
    "\n",
    "#print the weight matrices\n",
    "print(\"Weights Input-Hidden (wih):\\n\", wih_loaded)\n",
    "print(\"\\nWeights Hidden-Output (who):\\n\", who_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d23d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plot the image\n",
    "image_array = np.asfarray(all_values[1:]).reshape((28,28))\n",
    "matplotlib.pyplot.imshow(image_array, cmap='Greys', interpolation='None')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f60945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example calculations using the diagram below\n",
    "\n",
    "# 3-layer neural net\n",
    "# 3 input nodes, 3 hidden nodes, 3 output nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0645d8e",
   "metadata": {},
   "source": [
    "\n",
    "<div>\n",
    "    <img src=\"https://i.imgur.com/Fvj7t8A.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e5bfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "we encode the weights into matrices.\n",
    "the weights between the input/hidden layer in the above graph are encoded in the following way\n",
    "\n",
    "             [w_1,1, w_2,1, w_3,1]\n",
    "weights_ih = [w_1,2, w_2,2, w_3,2]\n",
    "             [w_1,3, w_2,3, w_3,3]      \n",
    "eg\n",
    "\n",
    "the weights between the input/hidden layer in the above graph are:\n",
    "\n",
    "[w_1,1 = 0.9,  w_2,1 = 0.3, w_3,1 = 0.4]\n",
    "[w_1,2 = 0.2,  w_2,2 = 0.8, w_3,2 = 0.2]\n",
    "[w_1,3 = 0.1,  w_2,3 = 0.5, w_3,3 = 0.6]\n",
    "\n",
    "which is represented in the code below, along with the inputs/weights_ho\n",
    "(note: not all of the weights are shown in the diagram)\n",
    "'''\n",
    "inputs = np.array([[0.9], [0.1], [0.8]])\n",
    "weights_ih = np.array([[0.9, 0.3, 0.4], [0.2, 0.8, 0.2], [0.1, 0.5, 0.6]])\n",
    "weights_ho = np.array([[0.3, 0.7, 0.5], [0.6, 0.5, 0.2], [0.8, 0.1, 0.9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b62122",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hidden layer input calculatiom\n",
    "hidden_inputs = np.dot(weights_ih,inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089f1e88",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/6E4M5kK.png\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da3f804",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hidden_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffbf99b",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/SaFSRNZ.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8fca82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating hidden_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a84f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_function = lambda x: scipy.special.expit(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eabfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_outputs = activation_function(hidden_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e297a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd5e313",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/UvfNuFy.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc3e322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the output layer inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f38a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_inputs = np.dot(weights_ho,hidden_outputs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d490d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_layer_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1fa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the output layers output\n",
    "output_layer_outputs = activation_function(output_layer_inputs)\n",
    "output_layer_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c1222",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/h9ifRY5.png\" width=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12118ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the multivariate chain rule, we obtain the gradient descent algorithms\n",
    "# we dropped the constant 2 term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adc6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8f5b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_who(who, node_errors, final_outputs, hidden_outputs):\n",
    "    who += learning_rate * np.dot((node_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))\n",
    "    return who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bee2db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_wih(hidden_errors, hidden_outputs, inputs, wih):\n",
    "    wih += learning_rate * np.dot((hidden_errors * hidden_outputs * (1.0 - hidden_outputs)), np.transpose(inputs))\n",
    "    return wih"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2664b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's consider a simpler neural network to see how gradient descent is used to update the weight who_11\n",
    "# first we will need to see how to calculate the error terms, which are node_errors and hidden_errors\n",
    "# we will only need the node_errors term for our example, but we also show how to obtain hidden_errors for updating wih\n",
    "# the node_errors and hidden_errors terms are used to insert into the gradient descent algorithms for updating who and wih"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1400da3",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/2iCQMhS.png\" width=\"500\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9906ef8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node_errors is calculated as (target - actual). this is calculating the term that appears in\n",
    "# the gradient descent algorithm for updating the weights who. \n",
    "# note: this is distinct from calculating the loss, we are merely getting the term that appears in the gradient descent algorithm for who \n",
    "# in this example the calculation has already been done for us\n",
    "node_errors = np.array([[0.8], [0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf088fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to calculate the hidden_errors term that appears in the gradient descent algorithm for wih we do what is seen below\n",
    "# first we have to obtain the who matrix which is:\n",
    "who = np.array([[2.0/(2.0+3.0), 3.0/(3.0+2.0)], [1.0/(1.0+4.0), 4.0/(4.0+1.0)]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8befe76f",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/0lObJPw.png\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445e875a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next we tranpose the who matrix and multiply it by the node errors\n",
    "hidden_errors = np.dot(who.T, node_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713a85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the term that appears in the gradient descent algorithm for calculating wih\n",
    "hidden_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afc0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in this example we now have the node_error and hidden_error to insert into the gradient descent algorithms\n",
    "# note, the actual code drops the denominator/normalization terms that appear in the who matrix, whereas the handwritten example above includes it\n",
    "# from henceforth we will also drop the normalization \n",
    "# now that we have node_errors let's the updated weight who_1,1, which is currently equal to 2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e61436c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the relevant node error for who_1,1 is 0.8\n",
    "node_error = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff86d1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# before we move further we will review some background material: the book's derivation of the gradient descent algorithm is included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f61d2d8",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/bDAuYGs.png\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27edc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# and putting it all together with the learning rate (where we drop the constant term 2), we have:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa26d2",
   "metadata": {},
   "source": [
    "<div>\n",
    "    <img src=\"https://i.imgur.com/37P0suW.png\" width=\"400\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacce663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the old weight are adjusted by the negative of the error's slope\n",
    "# we want to decrease the new weight if we have a positive slope\n",
    "# and increase it if we have a negative slope\n",
    "# also note dE/dwjk has a negative sign\n",
    "# there is a distinct negative sign in front of the learning rate\n",
    "# these two negatives signs cancel each other out and do not appear in\n",
    "# the gradient descent algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae24d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_errors = np.array([[0.8], [0.5]])\n",
    "final_inputs = np.dot(who, hidden_outputs)\n",
    "final_inputs\n",
    "final_outputs = activation_function(final_inputs).reshape(2,1)\n",
    "learning_rate = 0.1\n",
    "who = np.array([[2.0, 3.0], [1.0, 4.0]])\n",
    "hidden_outputs = np.array([[0.4], [0.5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9366ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76774ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_outputs * (1-final_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "who += learning_rate * np.dot((node_errors * final_outputs * (1.0 - final_outputs)), np.transpose(hidden_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ec336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# our updated who_11 weight is below\n",
    "who[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06be4d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the rest of the who weight updates\n",
    "who"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
